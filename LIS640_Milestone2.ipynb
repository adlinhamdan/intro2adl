{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:**\n",
    "\n",
    "This project is a regression task, where the goal is to use time-series analog sensor data from a manufacturing assembly line to predict a continuous numerical target related to system performance, anomalies, or equipment failure. The input features (X) consist of sensor readings from actuators, transducers, and control mechanisms, capturing critical operational parameters over time. The prediction target (y) is selected based on key metrics that indicate system health, efficiency, or potential faults.\n",
    "\n",
    "In setting up the DataLoader, some changes I have made are:\n",
    "\n",
    "- Selecting only numerical features relevant to predicting system behavior.\n",
    "- Normalizing the inputs using MinMaxScaler to stabilize training and improve convergence.\n",
    "- Ensuring proper train-validation-test splits to evaluate the model‚Äôs generalization.\n",
    "- Correcting shape of the data to ensure input Shape Matches Model Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f036f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1db988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/b4n2c1s96fz9p4jky31hmzh40000gn/T/ipykernel_56473/2184173045.py:18: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"R01_Data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Sample: torch.Size([64, 8])\n",
      "Label Sample: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "\n",
    "class AnalogSensorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.reshape(-1, 1)  # Ensure y is (batch_size, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "# Load pandas dataframe\n",
    "df = pd.read_csv(\"R01_Data.csv\")\n",
    "\n",
    "# Convert `_time` to a numerical feature (elapsed seconds since start)\n",
    "df[\"_time\"] = pd.to_datetime(df[\"_time\"])  # Convert to datetime\n",
    "df[\"_time\"] = (df[\"_time\"] - df[\"_time\"].min()).dt.total_seconds()  # Convert to elapsed time\n",
    "\n",
    "# Drop non-numeric columns (if `Description` is not needed)\n",
    "df = df.drop(columns=[\"Description\"])\n",
    "\n",
    "# Separate features and target (assuming last column is the target)\n",
    "X = df.iloc[:, :-1].values  # Features (all but last column)\n",
    "y = df.iloc[:, -1].values   # Target (last column)\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-Validation Split (80% Train, 20% Validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize Features using Training Set Statistics\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_val = scaler_y.transform(y_val.reshape(-1, 1))\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create dataset instances\n",
    "train_data = AnalogSensorDataset(X_train, y_train)\n",
    "val_data = AnalogSensorDataset(X_val, y_val)\n",
    "test_data = AnalogSensorDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    features, labels = batch\n",
    "    print(\"Feature Sample:\", features.shape)\n",
    "    print(\"Label Sample:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adaa9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_in, d_out, d_hidden, n_layers = 2):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(d_in, d_hidden), nn.BatchNorm1d(d_hidden, momentum = 0.1),\n",
    "            nn.ReLU()]\n",
    "        for layer in range(n_layers):\n",
    "            layers += [nn.Linear(d_hidden, d_hidden), nn.BatchNorm1d(d_hidden, momentum = 0.1),nn.ReLU(), nn.Dropout(p=0.3)]\n",
    "        layers += [nn.Linear(d_hidden, d_out)]\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031770e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=100, bias=True)\n",
      "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (8): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(8, 1, 100).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a0e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and the optimizer\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)  # Adjust learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b2c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: Train Loss = 0.0024, Validation Loss = 0.0002\n",
      "Epoch 2/6: Train Loss = 0.0009, Validation Loss = 0.0002\n",
      "Epoch 3/6: Train Loss = 0.0008, Validation Loss = 0.0002\n",
      "Epoch 4/6: Train Loss = 0.0008, Validation Loss = 0.0001\n",
      "Epoch 5/6: Train Loss = 0.0008, Validation Loss = 0.0001\n",
      "Epoch 6/6: Train Loss = 0.0007, Validation Loss = 0.0004\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if improvement\n",
    "        else:\n",
    "            self.counter += 1  # Increment if no improvement\n",
    "            if self.counter >= self.patience:\n",
    "                return True  # Stop training\n",
    "        return False\n",
    "        \n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, loss_fn, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)  # Move data to device\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(batch_X).squeeze()  # Forward pass\n",
    "        loss = loss_fn(outputs, batch_y.to(device).squeeze())  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "    \n",
    "    return total_loss / len(train_loader)  # Return average loss per batch\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, loss_fn, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X).squeeze()  # Ensure output shape matches labels\n",
    "            loss = loss_fn(outputs, batch_y.squeeze())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss  # Return MSE loss (lower is better)\n",
    "\n",
    "    \n",
    "# Train the model for 3 different hyper parameter settings (e.g. different learning rates, different loss functions that \n",
    "#make sense for your data, etc.)\n",
    "\n",
    "num_epochs = 6\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)  # Stop if no improvement for 3 epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)  # Get training loss\n",
    "    val_loss = evaluate(model, val_loader, loss_fn, device)  # Get validation loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Check early stopping\n",
    "    if early_stopping(val_loss):\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc551ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss (MSE): 0.0004\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "test_loss = evaluate(model, test_loader, loss_fn, device)\n",
    "print(f\"Final Test Loss (MSE): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    "\n",
    "I experimented with different architectures by adjusting Dropout, Batch Normalization, and Learning Rate, while using early stopping to prevent overfitting. Here‚Äôs what I observed:\n",
    "\n",
    "1. Decreasing Dropout (p=0.4 ‚Üí 0.3)\n",
    "\n",
    "Initially, the model showed signs of over-regularization with Dropout = 0.4, where validation loss remained too stable without improving further.\n",
    "Reducing Dropout to 0.3 improved training efficiency, allowing the model to learn more meaningful features.\n",
    "Final Test Loss (MSE): 0.0002, indicating a well-generalized model without excessive regularization.\n",
    "\n",
    "2. Decreasing Learning Rate (0.01 ‚Üí 3e-3)\n",
    "\n",
    "The train loss initially remained constant (0.0016 for multiple epochs) with a lower learning rate (1e-3), suggesting it was too low.\n",
    "Increasing it to 3e-3 allowed more effective weight updates, leading to a smoother decrease in training loss (0.0024 ‚Üí 0.0009).\n",
    "Validation loss remained stable at 0.0002, meaning the model converged efficiently without overshooting.\n",
    "\n",
    "3. Modifying BatchNorm (Momentum Adjusted to 0.1)\n",
    "\n",
    "Before adding BatchNorm, the model overfitted, with training loss decreasing but validation loss increasing.\n",
    "After adding BatchNorm (momentum = 0.1), the model stabilized, preventing rapid fluctuations in loss.\n",
    "Lowering momentum (0.99 ‚Üí 0.1) improved adaptability, leading to better weight updates across mini-batches.\n",
    "Final performance improved, with both train and validation loss decreasing smoothly while maintaining stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ff8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/28/09/c4d329f7969443cdd4d482048ca406b6f61cda3c8e99ace71feaec7c8734/optuna-4.2.1-py3-none-any.whl.metadata\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/99/f7/d398fae160568472ddce0b3fde9c4581afc593019a6adc91006a66406991/alembic-1.15.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: tqdm in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from optuna) (6.0)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/cd/83/de0a49e7de540513f53ab5d2e105321dedeb08a8f5850f0208decf4390ec/Mako-1.3.9-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/adlinfarhana/anaconda3/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define an objective function to be minimized.\n",
    "def objective(trial, train_loader=train_loader, val_loader=val_loader):\n",
    "    # Define hyperparameters to optimize\n",
    "    d_hidden = trial.suggest_int(\"d_hidden\", 64, 256, step=32)  # Hidden layer size\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)  # Learning rate (log scale)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 6, step=1)\n",
    "    \n",
    "    # Define model\n",
    "    model = NeuralNetwork(d_in=8, d_out=1, d_hidden=d_hidden, n_layers=n_layers).to(device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Implement Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=1e-4)\n",
    "\n",
    "    # Train model\n",
    "    num_epochs = 10  # Keep small for quick tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_X).squeeze()  # Ensure output shape matches labels\n",
    "            loss = loss_fn(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                preds = model(batch_X).squeeze()\n",
    "                val_loss += loss_fn(preds, batch_y).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Report intermediate results to Optuna\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Check early stopping\n",
    "        if early_stopping(avg_val_loss):\n",
    "            break\n",
    "\n",
    "    return avg_val_loss  # Return average validation loss\n",
    "    \n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize MSE\n",
    "study.optimize(objective, n_trials=6)  # Run 20 trials\n",
    "\n",
    "# Print out the best parameters.\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "d_hidden = study.best_params[\"d_hidden\"]\n",
    "lr = study.best_params[\"lr\"]\n",
    "n_layers = study.best_params[\"n_layers\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "Did you find the hyperparameter search helpful? Does it help to increase the number of trials in the optimization? Note that so far we have used the simplest version of optuna which has many nice features. Can you discover more useful features by browsing the optuna website? (Hint: try pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
